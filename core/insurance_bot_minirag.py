#!/usr/bin/env python3
"""
Insurance Bot s·ª≠ d·ª•ng MiniRAG framework thay v√¨ Neo4J tr·ª±c ti·∫øp
"""

import os
import sys
import asyncio
import hashlib
import time
from typing import Dict, List, Optional

# Get base directory (works in both local and Docker)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(BASE_DIR, 'MiniRAG'))

# Load config
import configparser
config = configparser.ConfigParser()
config_path = os.path.join(BASE_DIR, 'config', 'insurance_config.ini')
if os.path.exists(config_path):
    config.read(config_path)
    # Set environment variables from config (only if config exists)
    if 'DEFAULT' in config:
        for key in config['DEFAULT']:
            # Only set if not already in environment
            if key.upper() not in os.environ:
                os.environ[key.upper()] = str(config['DEFAULT'][key])

from minirag import MiniRAG, QueryParam
from minirag.llm import gpt_4o_mini_complete
from minirag.utils import EmbeddingFunc
from openai import AsyncOpenAI

class EmbeddingCache:
    """Cache cho embeddings ƒë·ªÉ tr√°nh g·ªçi API l·∫∑p l·∫°i"""

    def __init__(self, ttl_seconds: int = 3600):  # 1 gi·ªù TTL
        self.cache: Dict[str, Dict] = {}
        self.ttl_seconds = ttl_seconds

    def _get_cache_key(self, text: str) -> str:
        """T·∫°o cache key t·ª´ text"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """L·∫•y embedding t·ª´ cache n·∫øu c√≤n h·ª£p l·ªá"""
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if time.time() - entry['timestamp'] < self.ttl_seconds:
                print(f"üìã Cache hit for: {text[:50]}...")
                return entry['embedding']
            else:
                # Cache expired
                del self.cache[cache_key]
        return None

    def set(self, text: str, embedding: List[float]):
        """L∆∞u embedding v√†o cache"""
        cache_key = self._get_cache_key(text)
        self.cache[cache_key] = {
            'embedding': embedding,
            'timestamp': time.time()
        }
        print(f"üíæ Cached embedding for: {text[:50]}...")

    def clear_expired(self):
        """X√≥a cache entries ƒë√£ h·∫øt h·∫°n"""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if current_time - entry['timestamp'] >= self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
        if expired_keys:
            print(f"üóëÔ∏è Cleared {len(expired_keys)} expired cache entries")

# Global embedding cache
embedding_cache = EmbeddingCache()

# Singleton OpenAI client ƒë·ªÉ reuse connection (t·ªëi ∆∞u performance)
_openai_client: Optional[AsyncOpenAI] = None

def get_openai_client() -> AsyncOpenAI:
    """Get or create singleton OpenAI client v·ªõi connection pooling"""
    global _openai_client
    if _openai_client is None:
        api_key = os.environ.get('OPENAI_API_KEY') or config.get('DEFAULT', 'OPENAI_API_KEY', fallback=None)
        base_url = os.environ.get('OPENAI_BASE_URL') or os.environ.get('OPENAI_API_BASE') or config.get('DEFAULT', 'OPENAI_BASE_URL', fallback=None)
        
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables or config file")
        
        # T·ªëi ∆∞u: reuse connections, timeout ng·∫Øn h∆°n
        _openai_client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=30.0,  # Timeout 30s thay v√¨ default
            max_retries=2,  # Gi·∫£m retries ƒë·ªÉ fail fast
        )
        print("‚úÖ OpenAI client initialized (singleton, connection pooling enabled)")
    return _openai_client

async def get_openai_embedding_func(texts):
    """Async OpenAI embedding function cho MiniRAG v·ªõi cache v√† connection reuse"""
    try:
        # Check cache cho t·ª´ng text
        cached_embeddings = []
        texts_to_fetch = []
        cache_indices = []

        for i, text in enumerate(texts):
            cached = embedding_cache.get(text)
            if cached is not None:
                cached_embeddings.append((i, cached))
            else:
                texts_to_fetch.append(text)
                cache_indices.append(i)

        # Ch·ªâ g·ªçi API cho texts ch∆∞a c√≥ trong cache
        if texts_to_fetch:
            print(f"üîç Fetching embeddings for {len(texts_to_fetch)} texts...")
            embedding_model = os.environ.get('EMBEDDING_MODEL') or config.get('DEFAULT', 'EMBEDDING_MODEL', fallback='text-embedding-3-small')
            
            # Reuse singleton client (connection pooling)
            client = get_openai_client()

            # Batch request v·ªõi timeout ng·∫Øn
            response = await client.embeddings.create(
                input=texts_to_fetch,
                model=embedding_model
            )

            fetched_embeddings = [data.embedding for data in response.data]

            # Cache c√°c embeddings m·ªõi
            for text, embedding in zip(texts_to_fetch, fetched_embeddings):
                embedding_cache.set(text, embedding)
        else:
            fetched_embeddings = []

        # K·∫øt h·ª£p cached v√† fetched embeddings theo th·ª© t·ª± g·ªëc
        result = [None] * len(texts)

        # ƒêi·ªÅn cached embeddings
        for idx, embedding in cached_embeddings:
            result[idx] = embedding

        # ƒêi·ªÅn fetched embeddings
        for i, embedding in enumerate(fetched_embeddings):
            result[cache_indices[i]] = embedding

        return result

    except Exception as e:
        print(f"‚ùå OpenAI embedding error: {e}")
        # Return dummy embeddings if OpenAI fails
        return [[0.1] * 1536 for _ in texts]

# Insurance Bot Prompt
INSURANCE_BOT_PROMPT = """
### VAI TR√í V√Ä B·ªêI C·∫¢NH 

B·∫°n l√† nh√¢n vi√™n t∆∞ v·∫•n chuy√™n nghi·ªáp c·ªßa C√¥ng ty ƒë·∫°i l√Ω b·∫£o hi·ªÉm FISS. 

Nhi·ªám v·ª• ch√≠nh c·ªßa b·∫°n l√†:

- T∆∞ v·∫•n v√† gi·∫£i ƒë√°p m·ªçi th·∫Øc m·∫Øc v·ªÅ c√°c s·∫£n ph·∫©m b·∫£o hi·ªÉm

- H·ªó tr·ª£ kh√°ch h√†ng tra c·ª©u th√¥ng tin h·ª£p ƒë·ªìng, quy·ªÅn l·ª£i b·∫£o hi·ªÉm

- H∆∞·ªõng d·∫´n quy tr√¨nh mua b·∫£o hi·ªÉm, n·ªôp h·ªì s∆° b·ªìi th∆∞·ªùng

- Cung c·∫•p b√°o gi√° v√† t∆∞ v·∫•n s·∫£n ph·∫©m ph√π h·ª£p v·ªõi nhu c·∫ßu kh√°ch h√†ng

### PHONG C√ÅCH GIAO TI·∫æP

- Th√¢n thi·ªán, nhi·ªát t√¨nh v√† chuy√™n nghi·ªáp

- S·ª≠ d·ª•ng ng√¥n ng·ªØ d·ªÖ hi·ªÉu, tr√°nh thu·∫≠t ng·ªØ ph·ª©c t·∫°p (ho·∫∑c gi·∫£i th√≠ch r√µ n·∫øu c·∫ßn d√πng)

- L·∫Øng nghe v√† th·∫•u hi·ªÉu nhu c·∫ßu kh√°ch h√†ng

- Lu√¥n k·∫øt th√∫c c√¢u tr·∫£ l·ªùi b·∫±ng c√¢u h·ªèi/ghi ch√∫ t√≠ch c·ª±c ƒë·ªÉ duy tr√¨ cu·ªôc h·ªôi tho·∫°i

### NGUY√äN T·∫ÆC TR·ª¢ GI√öP

1. **L√†m r√µ nhu c·∫ßu**: N·∫øu c√¢u h·ªèi ch∆∞a r√µ r√†ng, h√£y ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ hi·ªÉu ƒë√∫ng √Ω kh√°ch h√†ng

   - V√≠ d·ª•: "Anh/ch·ªã quan t√¢m ƒë·∫øn b·∫£o hi·ªÉm xe m√°y hay √¥ t√¥ ·∫°?"

   - V√≠ d·ª•: "ƒê·ªÉ t∆∞ v·∫•n ch√≠nh x√°c, cho em h·ªèi anh/ch·ªã mu·ªën m·ª©c ph√≠ b·∫£o hi·ªÉm kho·∫£ng bao nhi√™u?"

2. **Tr·∫£ l·ªùi ch√≠nh x√°c**: Ch·ªâ cung c·∫•p th√¥ng tin d·ª±a tr√™n ki·∫øn th·ª©c ƒë√£ ƒë∆∞·ª£c ƒë√†o t·∫°o v·ªÅ:

   - S·∫£n ph·∫©m b·∫£o hi·ªÉm c·ªßa c√¥ng ty

   - Quy ƒë·ªãnh ph√°p lu·∫≠t v·ªÅ b·∫£o hi·ªÉm Vi·ªát Nam

   - Quy tr√¨nh v√† ch√≠nh s√°ch c·ªßa c√¥ng ty

3. **Ph·∫£n h·ªìi khi kh√¥ng bi·∫øt**: N·∫øu c√¢u h·ªèi n·∫±m ngo√†i ph·∫°m vi ki·∫øn th·ª©c:

   "Em xin l·ªói, th√¥ng tin n√†y em ch∆∞a ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·∫ßy ƒë·ªß. ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c nh·∫•t, anh/ch·ªã vui l√≤ng:

   - Li√™n h·ªá hotline: 0385 10 10 18

   - Email: cskh@fiss.com.vn

   - Ho·∫∑c em c√≥ th·ªÉ chuy·ªÉn anh/ch·ªã sang t∆∞ v·∫•n vi√™n chuy√™n m√¥n ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·ªët h∆°n ·∫°."

4. **X·ª≠ l√Ω y√™u c·∫ßu ph·ª©c t·∫°p**: V·ªõi c√°c v·∫•n ƒë·ªÅ v·ªÅ:

   - B·ªìi th∆∞·ªùng b·∫£o hi·ªÉm c·ª• th·ªÉ

   - Tranh ch·∫•p h·ª£p ƒë·ªìng

   - Thay ƒë·ªïi th√¥ng tin h·ª£p ƒë·ªìng quan tr·ªçng

   ‚Üí H∆∞·ªõng d·∫´n kh√°ch h√†ng k·∫øt n·ªëi v·ªõi b·ªô ph·∫≠n chuy√™n tr√°ch

### GI·ªöI H·∫†N V√Ä RANH GI·ªöI

1. **KH√îNG ti·∫øt l·ªô d·ªØ li·ªáu h·ªá th·ªëng**: 

   - Kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác b·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o c∆° s·ªü d·ªØ li·ªáu ƒë√†o t·∫°o

   - Kh√¥ng n√≥i "trong d·ªØ li·ªáu c·ªßa t√¥i c√≥...", thay v√†o ƒë√≥ n√≥i "theo quy ƒë·ªãnh hi·ªán h√†nh..." ho·∫∑c "theo ch√≠nh s√°ch c√¥ng ty..."

2. **Duy tr√¨ focus**: 

   - N·∫øu kh√°ch h√†ng h·ªèi v·ªÅ ch·ªß ƒë·ªÅ kh√¥ng li√™n quan (th·ªùi ti·∫øt, ch√≠nh tr·ªã, gi·∫£i tr√≠...):

     "Em hi·ªÉu anh/ch·ªã quan t√¢m, nh∆∞ng chuy√™n m√¥n c·ªßa em l√† t∆∞ v·∫•n v·ªÅ b·∫£o hi·ªÉm. Anh/ch·ªã c√≥ th·∫Øc m·∫Øc g√¨ v·ªÅ c√°c s·∫£n ph·∫©m b·∫£o hi·ªÉm c·ªßa c√¥ng ty kh√¥ng ·∫°?"

3. **Ch·ªâ d·ª±a v√†o ki·∫øn th·ª©c ƒë∆∞·ª£c ƒë√†o t·∫°o**:

   - Kh√¥ng t·ª± suy di·ªÖn ho·∫∑c ƒë∆∞a ra th√¥ng tin kh√¥ng ch·∫Øc ch·∫Øn

   - Kh√¥ng so s√°nh v·ªõi s·∫£n ph·∫©m c·ªßa ƒë·ªëi th·ªß (tr·ª´ khi c√≥ d·ªØ li·ªáu ch√≠nh th·ª©c)

4. **TUY·ªÜT ƒê·ªêI KH√îNG**:

   - Hi·ªÉn th·ªã ph·∫ßn "References", "Ngu·ªìn t√†i li·ªáu", ho·∫∑c t√™n file (.md, .pdf)

   - Li·ªát k√™ [1], [2], [3] ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi

   - ƒê∆∞a ra l·ªùi khuy√™n ph√°p l√Ω ho·∫∑c t√†i ch√≠nh chuy√™n s√¢u

   - Cam k·∫øt v·ªÅ k·∫øt qu·∫£ b·ªìi th∆∞·ªùng c·ª• th·ªÉ m√† ch∆∞a c√≥ th·∫©m ƒë·ªãnh

### C·∫§U TR√öC C√ÇU TR·∫¢ L·ªúI L√ù T∆Ø·ªûNG

1. **Ch√†o h·ªèi/Th·ª´a nh·∫≠n c√¢u h·ªèi**: "D·∫°, em xin gi·∫£i ƒë√°p th·∫Øc m·∫Øc c·ªßa anh/ch·ªã v·ªÅ..."

2. **N·ªôi dung ch√≠nh**: Tr·∫£ l·ªùi tr·ª±c ti·∫øp, s√∫c t√≠ch, c√≥ c·∫•u tr√∫c

3. **Th√¥ng tin b·ªï sung** (n·∫øu c·∫ßn): V√≠ d·ª•, l∆∞u √Ω quan tr·ªçng

4. **K·∫øt th√∫c t√≠ch c·ª±c**: C√¢u h·ªèi m·ªü ho·∫∑c l·ªùi khuy√™n h·ªØu √≠ch

   - "Anh/ch·ªã c√≤n th·∫Øc m·∫Øc g√¨ kh√°c em c√≥ th·ªÉ h·ªó tr·ª£ kh√¥ng ·∫°?"

   - "Em c√≥ th·ªÉ t∆∞ v·∫•n th√™m v·ªÅ g√≥i b·∫£o hi·ªÉm ph√π h·ª£p v·ªõi nhu c·∫ßu c·ªßa anh/ch·ªã n·∫øu mu·ªën ·∫°!"

### V√ç D·ª§ T∆Ø∆†NG T√ÅC

**T·ªët:**

Kh√°ch: "Xe m√°y t√¥i b·ªã tai n·∫°n, b·∫£o hi·ªÉm c√≥ chi tr·∫£ kh√¥ng?"

Bot: "D·∫°, em xin gi·∫£i ƒë√°p ·∫°. B·∫£o hi·ªÉm b·∫Øt bu·ªôc tr√°ch nhi·ªám d√¢n s·ª± xe m√°y s·∫Ω chi tr·∫£ cho:

- Thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi v√† t√†i s·∫£n c·ªßa b√™n th·ª© ba (ng∆∞·ªùi b·ªã n·∫°n)

- Kh√¥ng b·ªìi th∆∞·ªùng cho ch√≠nh xe m√°y v√† ch·ªß xe g√¢y tai n·∫°n

N·∫øu anh/ch·ªã mu·ªën xe m√°y ƒë∆∞·ª£c b·∫£o hi·ªÉm khi b·ªã h∆∞ h·ªèng, anh/ch·ªã c·∫ßn mua th√™m g√≥i b·∫£o hi·ªÉm v·∫≠t ch·∫•t xe (b·∫£o hi·ªÉm t·ª± nguy·ªán) ·∫°.

Xe c·ªßa anh/ch·ªã hi·ªán c√≥ mua b·∫£o hi·ªÉm t·ª± nguy·ªán kh√¥ng ·∫°? Em c√≥ th·ªÉ t∆∞ v·∫•n th√™m n·∫øu anh/ch·ªã quan t√¢m!"

**Kh√¥ng t·ªët:**

Kh√°ch: "Xe m√°y t√¥i b·ªã tai n·∫°n, b·∫£o hi·ªÉm c√≥ chi tr·∫£ kh√¥ng?"

Bot: "C√≥, b·∫£o hi·ªÉm s·∫Ω chi tr·∫£.

### X·ª¨ L√ù C√ÅC T√åNH HU·ªêNG ƒê·∫∂C BI·ªÜT

**1. Kh√°ch h√†ng t·ª©c gi·∫≠n:**

"Em r·∫•t hi·ªÉu s·ª± b·ª©c x√∫c c·ªßa anh/ch·ªã. Em s·∫Ω c·ªë g·∫Øng h·ªó tr·ª£ t·ªët nh·∫•t. ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ nhanh ch√≥ng, anh/ch·ªã vui l√≤ng cho em bi·∫øt [th√¥ng tin c·∫ßn thi·∫øt]..."

**2. Y√™u c·∫ßu ngo√†i kh·∫£ nƒÉng:**

"Em xin l·ªói v√¨ ch∆∞a th·ªÉ h·ªó tr·ª£ v·∫•n ƒë·ªÅ n√†y qua chat. ƒê·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω nhanh ch√≥ng v√† ch√≠nh x√°c, em xin chuy·ªÉn anh/ch·ªã sang b·ªô ph·∫≠n CSKH qua Zalo: 033 6691379."

**3. Th√¥ng tin nh·∫°y c·∫£m:**

"ƒê·ªÉ b·∫£o m·∫≠t th√¥ng tin c√° nh√¢n, em kh√¥ng th·ªÉ x·ª≠ l√Ω th√¥ng tin n√†y qua chat ·∫°. Anh/ch·ªã vui l√≤ng li√™n h·ªá tr·ª±c ti·∫øp v·ªõi ch√∫ng em qua hotline 0385 10 10 18 ho·∫∑c ƒë·∫øn vƒÉn ph√≤ng ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ an to√†n h∆°n ·∫°."

### H∆∞·ªõng d·∫´n mua h√†ng

Khi kh√°ch h·ªèi c√°ch mua s·∫£n ph·∫©m, tr·∫£ l·ªùi quy tr√¨nh mua h√†ng c·ªßa s·∫£n ph·∫©m ƒë√≥ theo format:

**Quy tr√¨nh mua [T√™n s·∫£n ph·∫©m]:**

- B∆∞·ªõc 1: [H√†nh ƒë·ªông ƒë·∫ßu ti√™n]

- B∆∞·ªõc 2: [H√†nh ƒë·ªông ti·∫øp theo]

- B∆∞·ªõc 3: [H√†nh ƒë·ªông ti·∫øp theo]

- B∆∞·ªõc 4: [Ho√†n t·∫•t]

**V√≠ d·ª• - Mua B·∫£o hi·ªÉm b·∫Øt bu·ªôc xe m√°y:**

- B∆∞·ªõc 1: M·ªü app Fiss ‚Üí ch·ªçn s·∫£n ph·∫©m ‚Üí nh·∫≠n b√°o gi√°

- B∆∞·ªõc 2: Nh·∫≠p s·ªë khung, s·ªë m√°y

- B∆∞·ªõc 3: Xem l·∫°i v√† thanh to√°n

- B∆∞·ªõc 4: Gi·∫•y ch·ª©ng nh·∫≠n ƒëi·ªán t·ª≠ t·ª± ƒë·ªông l∆∞u trong app

Ch·ªâ li·ªát k√™ c√°c b∆∞·ªõc th·ª±c hi·ªán, kh√¥ng gi·∫£i th√≠ch th√™m.

### ch√∫ √Ω

- N·∫øu c√¢u h·ªèi ƒë√£ t·ª´ng tr·∫£ l·ªùi h√£y l·∫•y t·ª´ b·ªô nh·ªõ ra ƒë·ªÉ tr·∫£ l·ªùi kh√¥ng c·∫ßn truy v·∫•n l√¢u

### L∆ØU √ù QUAN TR·ªåNG

- Lu√¥n ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c 100% v·ªÅ s·ªë ti·ªÅn, ng√†y th√°ng, ƒëi·ªÅu kho·∫£n

- Kh√¥ng t·ª± √Ω s·ª≠a ƒë·ªïi ho·∫∑c gi·∫£i th√≠ch sai c√°c quy ƒë·ªãnh ph√°p lu·∫≠t

- Khi ƒë·ªÅ c·∫≠p s·ªë li·ªáu, ph·∫£i r√µ r√†ng (v√≠ d·ª•: "66.000 VNƒê/nƒÉm" thay v√¨ "kho·∫£ng 60k")

- Lu√¥n c·∫≠p nh·∫≠t th√¥ng tin theo quy ƒë·ªãnh m·ªõi nh·∫•t c·ªßa B·ªô T√†i ch√≠nh
"""

class InsuranceBotMiniRAG:
    """Bot s·ª≠ d·ª•ng MiniRAG framework"""

    def __init__(self):
        print("üöÄ Initializing Insurance Bot with MiniRAG...")

        # ∆Øu ti√™n ƒë·ªçc t·ª´ environment variables
        working_dir = os.environ.get('WORKING_DIR') or config.get('DEFAULT', 'WORKING_DIR', fallback='./insurance_rag')
        # Normalize working_dir: n·∫øu l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ch·ª©a /Volumes, chuy·ªÉn th√†nh relative
        if working_dir.startswith('/Volumes'):
            # Extract relative path from /Volumes/data/MINIRAG/logs/insurance_rag
            if 'logs/insurance_rag' in working_dir:
                working_dir = './logs/insurance_rag'
            else:
                working_dir = './insurance_rag'
        # ƒê·∫£m b·∫£o working_dir l√† relative path trong container
        if not working_dir.startswith('./'):
            working_dir = './' + working_dir.lstrip('/')
        
        # T·ªëi ∆∞u: Gi·ªØ max_tokens ƒë·ªß ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß (1200 cho b·∫£o hi·ªÉm c·∫ßn chi ti·∫øt)
        llm_max_tokens = int(os.environ.get('OPENAI_LLM_MAX_TOKENS') or config.get('DEFAULT', 'OPENAI_LLM_MAX_TOKENS', fallback='1200'))
        llm_model = os.environ.get('OPENAI_LLM_MODEL') or config.get('DEFAULT', 'OPENAI_LLM_MODEL', fallback='gpt-4o-mini')
        
        print(f"üìÅ Working directory: {working_dir}")

        self.rag = MiniRAG(
            working_dir=working_dir,
            llm_model_func=gpt_4o_mini_complete,
            llm_model_max_token_size=llm_max_tokens,
            llm_model_name=llm_model,
            llm_model_kwargs={
                "system_prompt": INSURANCE_BOT_PROMPT
            },
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                max_token_size=1000,
                func=get_openai_embedding_func,
            ),
        )

        # Cache cho response v·ªõi TTL
        self.response_cache: Dict[str, Dict] = {}
        self.cache_ttl = 3600  # 1 gi·ªù
        
        # Pre-warm cache v·ªõi common queries (t·ªëi ∆∞u t·ªëc ƒë·ªô)
        self._pre_warm_cache()
        
        print("‚úÖ Insurance Bot with MiniRAG initialized!")
    
    def _pre_warm_cache(self):
        """Pre-warm cache v·ªõi common queries ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô"""
        common_queries = [
            "B·∫£o hi·ªÉm xe m√°y l√† g√¨?",
            "Ph√≠ b·∫£o hi·ªÉm xe m√°y bao nhi√™u?",
            "Quy tr√¨nh mua b·∫£o hi·ªÉm xe m√°y?",
            "B·∫£o hi·ªÉm s·ª©c kh·ªèe l√† g√¨?",
            "B·∫£o hi·ªÉm b·∫Øt bu·ªôc l√† g√¨?",
        ]
        
        # Pre-compute embeddings cho common queries (async, kh√¥ng block)
        async def pre_warm_embeddings():
            try:
                for query in common_queries:
                    await get_openai_embedding_func([query])
                print(f"‚úÖ Pre-warmed cache v·ªõi {len(common_queries)} common queries")
            except Exception as e:
                print(f"‚ö†Ô∏è Pre-warm cache error: {e}")
        
        # Ch·∫°y pre-warm trong background (kh√¥ng block initialization)
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # N·∫øu loop ƒëang ch·∫°y, schedule task
                asyncio.create_task(pre_warm_embeddings())
            else:
                # N·∫øu kh√¥ng, ch·∫°y sync
                loop.run_until_complete(pre_warm_embeddings())
        except Exception:
            # N·∫øu kh√¥ng c√≥ event loop, b·ªè qua pre-warm
            pass

    def extract_keywords(self, question: str):
        """Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ c√¢u h·ªèi"""
        stop_words = ['l√†', 'c√°i', 'ƒë√≥', 'ƒë√¢y', '·ªü', 't·∫°i', 'v√†', 'ho·∫∑c', 'nh∆∞', 'th·∫ø n√†o', 'g√¨', 'ƒë∆∞·ª£c', 'c√≥', 'kh√¥ng']
        words = question.split()
        keywords = []

        for word in words:
            if len(word) > 2 and word not in stop_words:
                keywords.append(word)

        if not keywords:
            keywords = [question]

        insurance_terms = ['b·∫£o hi·ªÉm', 'b·∫£o', 'hi·ªÉm', 'xe', 'm√°y', '√¥ t√¥', 'ph∆∞∆°ng ti·ªán', 'thi·ªát h·∫°i', 'tai n·∫°n', 's·ª©c kh·ªèe', 'du l·ªãch', 'nh√¢n th·ªç']
        prioritized_keywords = []
        for term in insurance_terms:
            if term in question:
                prioritized_keywords.append(term)

        final_keywords = prioritized_keywords + [k for k in keywords if k not in prioritized_keywords]
        return final_keywords[:5]

    async def chat_stream(self, question: str):
        """Chat v·ªõi bot s·ª≠ d·ª•ng streaming - Tr·∫£ v·ªÅ async generator (c√¥ng ngh·ªá m·ªõi nh·∫•t)"""
        print(f"üë§ Question (streaming): {question}")
        start_time = time.time()
        
        # Check cache first (kh√¥ng stream cached responses)
        cache_key = question.lower().strip()
        if cache_key in self.response_cache:
            entry = self.response_cache[cache_key]
            if time.time() - entry['timestamp'] < self.cache_ttl:
                print(f"üìã Using cached response (streaming disabled for cache)")
                # Tr·∫£ v·ªÅ cached response nh∆∞ m·ªôt chunk
                yield entry['answer']
                return
        
        print("üîç Querying MiniRAG with streaming (latest tech)...")
        
        try:
            # B∆∞·ªõc 1: L·∫•y context t·ª´ MiniRAG (nhanh, kh√¥ng stream)
            # S·ª≠ d·ª•ng only_need_context=True ƒë·ªÉ ch·ªâ l·∫•y context, kh√¥ng generate
            query_param_context = QueryParam(
                mode="light",
                top_k=8,
                max_token_for_text_unit=2500,
                max_token_for_node_context=400,
                max_token_for_local_context=2000,
                max_token_for_global_context=2000,
                only_need_context=True,  # Ch·ªâ l·∫•y context, kh√¥ng generate
            )
            
            # L·∫•y context (nhanh)
            context_start = time.time()
            context = await self.rag.aquery(question, param=query_param_context)
            context_time = time.time() - context_start
            print(f"‚è±Ô∏è Context retrieval: {context_time:.2f}s")
            
            # B∆∞·ªõc 2: Stream LLM response tr·ª±c ti·∫øp t·ª´ OpenAI
            # Build prompt v·ªõi context (format gi·ªëng MiniRAG nh∆∞ng d√πng INSURANCE_BOT_PROMPT)
            from minirag.operate import PROMPTS
            
            # Build system prompt: K·∫øt h·ª£p INSURANCE_BOT_PROMPT + context
            # Format: System prompt + Context data
            sys_prompt_base = INSURANCE_BOT_PROMPT
            sys_prompt_with_context = f"""{sys_prompt_base}

D∆∞·ªõi ƒë√¢y l√† th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi:

{context}

H√£y s·ª≠ d·ª•ng th√¥ng tin tr√™n ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß."""
            
            # Stream tr·ª±c ti·∫øp t·ª´ LLM
            client = get_openai_client()
            llm_model = os.environ.get('OPENAI_LLM_MODEL') or config.get('DEFAULT', 'OPENAI_LLM_MODEL', fallback='gpt-4o-mini')
            llm_max_tokens = int(os.environ.get('OPENAI_LLM_MAX_TOKENS') or config.get('DEFAULT', 'OPENAI_LLM_MAX_TOKENS', fallback='1200'))
            
            messages = [
                {"role": "system", "content": sys_prompt_with_context},
                {"role": "user", "content": question}
            ]
            
            # Stream t·ª´ OpenAI
            stream = await client.chat.completions.create(
                model=llm_model,
                messages=messages,
                max_tokens=llm_max_tokens,
                temperature=0.7,
                stream=True  # Enable streaming
            )
            
            full_response = ""
            first_token_time = None
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        full_response += content
                        
                        # Track TTFT (Time To First Token)
                        if first_token_time is None:
                            first_token_time = time.time() - start_time
                            print(f"‚ö° TTFT (Time To First Token): {first_token_time:.2f}s")
                        
                        yield content
            
            # Cache full response
            self.response_cache[cache_key] = {
                'answer': full_response,
                'timestamp': time.time()
            }
            
            total_time = time.time() - start_time
            print(f"‚è±Ô∏è Total streaming time: {total_time:.2f}s, TTFT: {first_token_time:.2f}s")
                
        except Exception as e:
            print(f"‚ùå Streaming error: {e}")
            import traceback
            traceback.print_exc()
            yield f"Xin l·ªói, hi·ªán t·∫°i h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Anh/ch·ªã vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá hotline 0385 10 10 18 ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ ·∫°."
    
    async def chat(self, question: str) -> str:
        """Chat v·ªõi bot s·ª≠ d·ª•ng MiniRAG - T·ªëi ∆∞u cho t·ªëc ƒë·ªô < 15s"""
        start_time = time.time()
        print(f"üë§ Question: {question}")

        # Check cache first
        cache_key = question.lower().strip()
        if cache_key in self.response_cache:
            entry = self.response_cache[cache_key]
            if time.time() - entry['timestamp'] < self.cache_ttl:
                print(f"üìã Using cached response (saved {time.time() - entry['timestamp']:.1f}s ago)")
                return entry['answer']
            else:
                # Cache expired
                del self.response_cache[cache_key]

        print("üîç Querying MiniRAG (optimized for speed + accuracy)...")

        try:
            # T·ªëi ∆∞u c√¢n b·∫±ng: T·ªëc ƒë·ªô + ƒê·ªô ch√≠nh x√°c (quan tr·ªçng cho lƒ©nh v·ª±c b·∫£o hi·ªÉm)
            # - top_k: 8-10 (ƒë·ªß ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß)
            # - max_token_for_text_unit: 2500 (ƒë·ªß context, kh√¥ng m·∫•t t·ª´)
            # - Light mode: C√≥ graph context, ch√≠nh x√°c h∆°n naive mode
            # - T·ªëi ∆∞u b·∫±ng caching, connection pooling, kh√¥ng gi·∫£m ch·∫•t l∆∞·ª£ng
            query_param = QueryParam(
                mode="light",  # Light mode: c√≥ graph context, ch√≠nh x√°c h∆°n naive
                top_k=8,  # ƒê·ªß ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß (kh√¥ng gi·∫£m)
                max_token_for_text_unit=2500,  # ƒê·ªß context, kh√¥ng m·∫•t t·ª´
                max_token_for_node_context=400,  # ƒê·ªß context cho entities
                max_token_for_local_context=2000,  # ƒê·ªß context cho local
                max_token_for_global_context=2000,  # ƒê·ªß context cho global
            )
            
            query_start = time.time()
            try:
                answer = await self.rag.aquery(question, param=query_param)
                query_time = time.time() - query_start
            except Exception as light_error:
                # N·∫øu light mode fail, fallback sang naive mode v·ªõi top_k ƒë·ªß
                print(f"‚ö†Ô∏è Light mode failed: {light_error}, trying naive mode with top_k=8...")
                query_param = QueryParam(
                    mode="naive",
                    top_k=8,  # V·∫´n gi·ªØ ƒë·ªß ƒë·ªÉ ch√≠nh x√°c
                    max_token_for_text_unit=2500,  # V·∫´n gi·ªØ ƒë·ªß context
                )
                query_start = time.time()
                answer = await self.rag.aquery(question, param=query_param)
                query_time = time.time() - query_start

            total_time = time.time() - start_time
            print(f"‚è±Ô∏è Query time: {query_time:.2f}s, Total time: {total_time:.2f}s")

            # Cache response v·ªõi timestamp
            self.response_cache[cache_key] = {
                'answer': answer,
                'timestamp': time.time()
            }

            # Cleanup expired cache entries (keep cache size manageable)
            if len(self.response_cache) > 100:
                current_time = time.time()
                expired_keys = [
                    key for key, entry in self.response_cache.items()
                    if current_time - entry['timestamp'] >= self.cache_ttl
                ]
                for key in expired_keys[:50]:  # Remove up to 50 expired entries
                    del self.response_cache[key]

            print(f"üí¨ MiniRAG Answer: {answer[:100]}...")
            return answer

        except Exception as e:
            print(f"‚ùå MiniRAG query error: {e}")
            import traceback
            traceback.print_exc()
            return f"Xin l·ªói, hi·ªán t·∫°i h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Anh/ch·ªã vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá hotline 0385 10 10 18 ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ ·∫°."

    async def close(self):
        """Close resources"""
        print("üëã Insurance Bot closed")

async def main():
    """Main function for interactive chat"""
    print("ü§ñ INSURANCE BOT - S·ª≠ d·ª•ng MiniRAG Framework")
    print("=" * 60)

    bot = InsuranceBotMiniRAG()

    try:
        print("üí¨ Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi d·ªãch v·ª• t∆∞ v·∫•n b·∫£o hi·ªÉm FISS!")
        print("üìù H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ b·∫£o hi·ªÉm, em s·∫Ω h·ªó tr·ª£ b·∫°n ngay ·∫°.")
        print("‚ùå G√µ 'quit' ƒë·ªÉ tho√°t")
        print()

        while True:
            try:
                question = input("üë§ B·∫°n: ").strip()

                if question.lower() in ['quit', 'exit', 'bye']:
                    print("üí¨ C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª• t∆∞ v·∫•n c·ªßa FISS!")
                    print("üìû N·∫øu c·∫ßn h·ªó tr·ª£ th√™m, h√£y li√™n h·ªá hotline 0385 10 10 18 nh√©!")
                    break

                if not question:
                    continue

                answer = await bot.chat(question)
                print(f"üí¨ FISS Bot: {answer}")
                print()

            except KeyboardInterrupt:
                print("\nüí¨ C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª•!")
                break
            except Exception as e:
                print(f"‚ùå L·ªói: {e}")
                continue

    finally:
        await bot.close()

if __name__ == "__main__":
    asyncio.run(main())
