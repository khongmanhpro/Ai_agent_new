#!/usr/bin/env python3
"""
Insurance Bot sá»­ dá»¥ng MiniRAG framework thay vÃ¬ Neo4J trá»±c tiáº¿p
"""

import os
import sys
import asyncio
import hashlib
import time
from typing import Dict, List, Optional

# Get base directory (works in both local and Docker)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(BASE_DIR, 'MiniRAG'))

# Load config
import configparser
config = configparser.ConfigParser()
config_path = os.path.join(BASE_DIR, 'config', 'insurance_config.ini')
if os.path.exists(config_path):
    config.read(config_path)
    # Set environment variables from config (only if config exists)
    if 'DEFAULT' in config:
        for key in config['DEFAULT']:
            # Only set if not already in environment
            if key.upper() not in os.environ:
                os.environ[key.upper()] = str(config['DEFAULT'][key])

from minirag import MiniRAG, QueryParam
from minirag.llm import gpt_4o_mini_complete
from minirag.utils import EmbeddingFunc
from openai import AsyncOpenAI

class EmbeddingCache:
    """Cache cho embeddings Ä‘á»ƒ trÃ¡nh gá»i API láº·p láº¡i"""

    def __init__(self, ttl_seconds: int = 3600):  # 1 giá» TTL
        self.cache: Dict[str, Dict] = {}
        self.ttl_seconds = ttl_seconds

    def _get_cache_key(self, text: str) -> str:
        """Táº¡o cache key tá»« text"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """Láº¥y embedding tá»« cache náº¿u cÃ²n há»£p lá»‡"""
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if time.time() - entry['timestamp'] < self.ttl_seconds:
                print(f"ğŸ“‹ Cache hit for: {text[:50]}...")
                return entry['embedding']
            else:
                # Cache expired
                del self.cache[cache_key]
        return None

    def set(self, text: str, embedding: List[float]):
        """LÆ°u embedding vÃ o cache"""
        cache_key = self._get_cache_key(text)
        self.cache[cache_key] = {
            'embedding': embedding,
            'timestamp': time.time()
        }
        print(f"ğŸ’¾ Cached embedding for: {text[:50]}...")

    def clear_expired(self):
        """XÃ³a cache entries Ä‘Ã£ háº¿t háº¡n"""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if current_time - entry['timestamp'] >= self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
        if expired_keys:
            print(f"ğŸ—‘ï¸ Cleared {len(expired_keys)} expired cache entries")

# Global embedding cache
embedding_cache = EmbeddingCache()

async def get_openai_embedding_func(texts):
    """Async OpenAI embedding function cho MiniRAG vá»›i cache"""
    try:
        # Check cache cho tá»«ng text
        cached_embeddings = []
        texts_to_fetch = []
        cache_indices = []

        for i, text in enumerate(texts):
            cached = embedding_cache.get(text)
            if cached is not None:
                cached_embeddings.append((i, cached))
            else:
                texts_to_fetch.append(text)
                cache_indices.append(i)

        # Chá»‰ gá»i API cho texts chÆ°a cÃ³ trong cache
        if texts_to_fetch:
            print(f"ğŸ” Fetching embeddings for {len(texts_to_fetch)} texts...")
            # Æ¯u tiÃªn Ä‘á»c tá»« environment variables, náº¿u khÃ´ng cÃ³ thÃ¬ Ä‘á»c tá»« config
            api_key = os.environ.get('OPENAI_API_KEY') or config.get('DEFAULT', 'OPENAI_API_KEY', fallback=None)
            base_url = os.environ.get('OPENAI_BASE_URL') or os.environ.get('OPENAI_API_BASE') or config.get('DEFAULT', 'OPENAI_BASE_URL', fallback=None)
            embedding_model = os.environ.get('EMBEDDING_MODEL') or config.get('DEFAULT', 'EMBEDDING_MODEL', fallback='text-embedding-3-small')
            
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment variables or config file")
            
            client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url
            )

            response = await client.embeddings.create(
                input=texts_to_fetch,
                model=embedding_model
            )

            fetched_embeddings = [data.embedding for data in response.data]

            # Cache cÃ¡c embeddings má»›i
            for text, embedding in zip(texts_to_fetch, fetched_embeddings):
                embedding_cache.set(text, embedding)
        else:
            fetched_embeddings = []

        # Káº¿t há»£p cached vÃ  fetched embeddings theo thá»© tá»± gá»‘c
        result = [None] * len(texts)

        # Äiá»n cached embeddings
        for idx, embedding in cached_embeddings:
            result[idx] = embedding

        # Äiá»n fetched embeddings
        for i, embedding in enumerate(fetched_embeddings):
            result[cache_indices[i]] = embedding

        return result

    except Exception as e:
        print(f"âŒ OpenAI embedding error: {e}")
        # Return dummy embeddings if OpenAI fails
        return [[0.1] * 1536 for _ in texts]

# Insurance Bot Prompt
INSURANCE_BOT_PROMPT = """
Báº¡n lÃ  nhÃ¢n viÃªn tÆ° váº¥n chuyÃªn nghiá»‡p cá»§a CÃ´ng ty Ä‘áº¡i lÃ½ báº£o hiá»ƒm FISS.

Nhiá»‡m vá»¥ chÃ­nh cá»§a báº¡n lÃ :
- TÆ° váº¥n vÃ  giáº£i Ä‘Ã¡p má»i tháº¯c máº¯c vá» cÃ¡c sáº£n pháº©m báº£o hiá»ƒm
- Há»— trá»£ khÃ¡ch hÃ ng tra cá»©u thÃ´ng tin há»£p Ä‘á»“ng, quyá»n lá»£i báº£o hiá»ƒm
- HÆ°á»›ng dáº«n quy trÃ¬nh mua báº£o hiá»ƒm, ná»™p há»“ sÆ¡ bá»“i thÆ°á»ng
- Cung cáº¥p bÃ¡o giÃ¡ vÃ  tÆ° váº¥n sáº£n pháº©m phÃ¹ há»£p vá»›i nhu cáº§u khÃ¡ch hÃ ng

Phong cÃ¡ch giao tiáº¿p:
- ThÃ¢n thiá»‡n, nhiá»‡t tÃ¬nh vÃ  chuyÃªn nghiá»‡p
- Sá»­ dá»¥ng ngÃ´n ngá»¯ dá»… hiá»ƒu, trÃ¡nh thuáº­t ngá»¯ phá»©c táº¡p
- Láº¯ng nghe vÃ  tháº¥u hiá»ƒu nhu cáº§u khÃ¡ch hÃ ng
- LuÃ´n káº¿t thÃºc cÃ¢u tráº£ lá»i báº±ng cÃ¢u há»i/ghi chÃº tÃ­ch cá»±c

NguyÃªn táº¯c:
- Tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn kiáº¿n thá»©c cÃ³ sáºµn
- KhÃ´ng Ä‘á» cáº­p Ä‘áº¿n nguá»“n tÃ i liá»‡u hay database
- Náº¿u khÃ´ng biáº¿t, hÆ°á»›ng dáº«n liÃªn há»‡ bá»™ pháº­n chuyÃªn mÃ´n
"""

class InsuranceBotMiniRAG:
    """Bot sá»­ dá»¥ng MiniRAG framework"""

    def __init__(self):
        print("ğŸš€ Initializing Insurance Bot with MiniRAG...")

        # Æ¯u tiÃªn Ä‘á»c tá»« environment variables
        working_dir = os.environ.get('WORKING_DIR') or config.get('DEFAULT', 'WORKING_DIR', fallback='./insurance_rag')
        # Normalize working_dir: náº¿u lÃ  Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i chá»©a /Volumes, chuyá»ƒn thÃ nh relative
        if working_dir.startswith('/Volumes'):
            # Extract relative path from /Volumes/data/MINIRAG/logs/insurance_rag
            if 'logs/insurance_rag' in working_dir:
                working_dir = './logs/insurance_rag'
            else:
                working_dir = './insurance_rag'
        # Äáº£m báº£o working_dir lÃ  relative path trong container
        if not working_dir.startswith('./'):
            working_dir = './' + working_dir.lstrip('/')
        
        llm_max_tokens = int(os.environ.get('OPENAI_LLM_MAX_TOKENS') or config.get('DEFAULT', 'OPENAI_LLM_MAX_TOKENS', fallback='1000'))
        llm_model = os.environ.get('OPENAI_LLM_MODEL') or config.get('DEFAULT', 'OPENAI_LLM_MODEL', fallback='gpt-4o-mini')
        
        print(f"ğŸ“ Working directory: {working_dir}")

        self.rag = MiniRAG(
            working_dir=working_dir,
            llm_model_func=gpt_4o_mini_complete,
            llm_model_max_token_size=llm_max_tokens,
            llm_model_name=llm_model,
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                max_token_size=1000,
                func=get_openai_embedding_func,
            ),
        )

        # Cache cho response
        self.response_cache = {}
        print("âœ… Insurance Bot with MiniRAG initialized!")

    def extract_keywords(self, question: str):
        """TrÃ­ch xuáº¥t tá»« khÃ³a tá»« cÃ¢u há»i"""
        stop_words = ['lÃ ', 'cÃ¡i', 'Ä‘Ã³', 'Ä‘Ã¢y', 'á»Ÿ', 'táº¡i', 'vÃ ', 'hoáº·c', 'nhÆ°', 'tháº¿ nÃ o', 'gÃ¬', 'Ä‘Æ°á»£c', 'cÃ³', 'khÃ´ng']
        words = question.split()
        keywords = []

        for word in words:
            if len(word) > 2 and word not in stop_words:
                keywords.append(word)

        if not keywords:
            keywords = [question]

        insurance_terms = ['báº£o hiá»ƒm', 'báº£o', 'hiá»ƒm', 'xe', 'mÃ¡y', 'Ã´ tÃ´', 'phÆ°Æ¡ng tiá»‡n', 'thiá»‡t háº¡i', 'tai náº¡n', 'sá»©c khá»e', 'du lá»‹ch', 'nhÃ¢n thá»']
        prioritized_keywords = []
        for term in insurance_terms:
            if term in question:
                prioritized_keywords.append(term)

        final_keywords = prioritized_keywords + [k for k in keywords if k not in prioritized_keywords]
        return final_keywords[:5]

    async def chat(self, question: str) -> str:
        """Chat vá»›i bot sá»­ dá»¥ng MiniRAG"""
        print(f"ğŸ‘¤ Question: {question}")

        # Check cache first
        cache_key = question.lower().strip()
        if cache_key in self.response_cache:
            print("ğŸ“‹ Using cached response")
            return self.response_cache[cache_key]

        print("ğŸ” Querying MiniRAG...")

        try:
            # Query MiniRAG
            answer = await self.rag.aquery(question, param=QueryParam(mode="mini"))

            # Cache response
            self.response_cache[cache_key] = answer

            print(f"ğŸ’¬ MiniRAG Answer: {answer[:100]}...")
            return answer

        except Exception as e:
            print(f"âŒ MiniRAG query error: {e}")
            return f"Xin lá»—i, hiá»‡n táº¡i há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Anh/chá»‹ vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ hotline 0385 10 10 18 Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ áº¡."

    async def close(self):
        """Close resources"""
        print("ğŸ‘‹ Insurance Bot closed")

async def main():
    """Main function for interactive chat"""
    print("ğŸ¤– INSURANCE BOT - Sá»­ dá»¥ng MiniRAG Framework")
    print("=" * 60)

    bot = InsuranceBotMiniRAG()

    try:
        print("ğŸ’¬ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i dá»‹ch vá»¥ tÆ° váº¥n báº£o hiá»ƒm FISS!")
        print("ğŸ“ HÃ£y Ä‘áº·t cÃ¢u há»i vá» báº£o hiá»ƒm, em sáº½ há»— trá»£ báº¡n ngay áº¡.")
        print("âŒ GÃµ 'quit' Ä‘á»ƒ thoÃ¡t")
        print()

        while True:
            try:
                question = input("ğŸ‘¤ Báº¡n: ").strip()

                if question.lower() in ['quit', 'exit', 'bye']:
                    print("ğŸ’¬ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥ tÆ° váº¥n cá»§a FISS!")
                    print("ğŸ“ Náº¿u cáº§n há»— trá»£ thÃªm, hÃ£y liÃªn há»‡ hotline 0385 10 10 18 nhÃ©!")
                    break

                if not question:
                    continue

                answer = await bot.chat(question)
                print(f"ğŸ’¬ FISS Bot: {answer}")
                print()

            except KeyboardInterrupt:
                print("\nğŸ’¬ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")
                continue

    finally:
        await bot.close()

if __name__ == "__main__":
    asyncio.run(main())
