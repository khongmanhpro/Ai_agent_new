#!/usr/bin/env python3
"""
∆Ø·ªõc t√≠nh chi ph√≠ token cho m·ªôt c√¢u h·ªèi
"""

import os
import sys
import asyncio
import configparser
from openai import AsyncOpenAI

# Load config
config = configparser.ConfigParser()
config.read('/Volumes/data/MINIRAG/config/insurance_config.ini')

for key in config['DEFAULT']:
    os.environ[key.upper()] = str(config['DEFAULT'][key])

sys.path.append('/Volumes/data/MINIRAG/MiniRAG')
from insurance_bot_minirag import InsuranceBotMiniRAG

async def estimate_token_cost():
    """∆Ø·ªõc t√≠nh chi ph√≠ token"""
    print("üßÆ ESTIMATING TOKEN COST FOR ONE QUESTION...")

    # Test v·ªõi c√¢u h·ªèi ƒë∆°n gi·∫£n
    question = "B·∫£o hi·ªÉm xe m√°y l√† g√¨?"

    # Kh·ªüi t·∫°o OpenAI client ƒë·ªÉ test tr·ª±c ti·∫øp
    client = AsyncOpenAI(
        api_key=config.get('DEFAULT', 'OPENAI_API_KEY'),
        base_url=config.get('DEFAULT', 'OPENAI_API_BASE')
    )

    try:
        print(f"‚ùì Question: {question}")

        # Test embedding cost (∆∞·ªõc t√≠nh)
        print("\nüîç Testing Embedding...")
        embed_response = await client.embeddings.create(
            input=[question],
            model="text-embedding-3-small"
        )
        embedding_tokens = embed_response.usage.total_tokens
        print(f"  Embedding tokens used: {embedding_tokens}")

        # Test LLM cost (∆∞·ªõc t√≠nh v·ªõi prompt ng·∫Øn)
        print("\nü§ñ Testing LLM...")
        system_prompt = """
        B·∫°n l√† nh√¢n vi√™n t∆∞ v·∫•n chuy√™n nghi·ªáp c·ªßa C√¥ng ty ƒë·∫°i l√Ω b·∫£o hi·ªÉm FISS.
        Tr·∫£ l·ªùi ng·∫Øn g·ªçn v·ªÅ b·∫£o hi·ªÉm xe m√°y.
        """

        llm_response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            max_tokens=1000,
            temperature=0.7
        )

        llm_input_tokens = llm_response.usage.prompt_tokens
        llm_output_tokens = llm_response.usage.completion_tokens
        total_llm_tokens = llm_response.usage.total_tokens

        print(f"  LLM input tokens: {llm_input_tokens}")
        print(f"  LLM output tokens: {llm_output_tokens}")
        print(f"  LLM total tokens: {total_llm_tokens}")

        # T√≠nh chi ph√≠
        embedding_cost = (embedding_tokens / 1_000_000) * 0.02  # $0.02 per 1M tokens
        llm_input_cost = (llm_input_tokens / 1_000_000) * 0.15   # $0.15 per 1M input tokens
        llm_output_cost = (llm_output_tokens / 1_000_000) * 0.60  # $0.60 per 1M output tokens

        total_cost = embedding_cost + llm_input_cost + llm_output_cost

        print("\nüí∞ COST BREAKDOWN PER QUESTION:")
        print(f"  Embedding: ${embedding_cost:.6f} ({embedding_tokens} tokens)")
        print(f"  LLM Input: ${llm_input_cost:.6f} ({llm_input_tokens} tokens)")
        print(f"  LLM Output: ${llm_output_cost:.6f} ({llm_output_tokens} tokens)")
        print(f"  TOTAL: ${total_cost:.6f}")

        # ∆Ø·ªõc t√≠nh cho nhi·ªÅu c√¢u h·ªèi
        print("\nüìà ESTIMATES:")
        print(f"  100 questions: ${total_cost * 100:.4f}")
        print(f"  1,000 questions: ${total_cost * 1000:.4f}")
        print(f"  10,000 questions: ${total_cost * 10000:.2f}")

        # Trong th·ª±c t·∫ø v·ªõi MiniRAG, c√≥ nhi·ªÅu embedding calls h∆°n
        print("\n‚ö†Ô∏è  L∆ØU √ù:")
        print("  - MiniRAG th·ª±c hi·ªán nhi·ªÅu embedding queries ƒë·ªÉ t√¨m context")
        print("  - Chi ph√≠ th·ª±c t·∫ø c√≥ th·ªÉ cao h∆°n 2-3 l·∫ßn")
        print("  - Gi√° tr√™n l√† cho text-embedding-3-small + gpt-4o-mini")

        print("\nüí¨ Sample Answer:")
        print(llm_response.choices[0].message.content[:200] + "...")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(estimate_token_cost())
