#!/usr/bin/env python3
"""
Script import t·∫•t c·∫£ vƒÉn b·∫£n ph√°p lu·∫≠t b·∫£o hi·ªÉm v√†o Neo4J

Import 6 files:
1. 04-2021-TTBTC_B·∫£o hi·ªÉm b·∫Øt bu·ªôc xe c∆° gi∆°i.md
2. MIC - Quy t·∫Øc b·∫£o hi·ªÉm du l·ªãch trong n∆∞·ªõc.md
3. MIC_CARE_Quy_tac_bao_hiem_suc_khoe_toan_dien_2025.md
4. MIC_Quy-tac-BH-Tai-nan-con-nguoi_2025.md
5. MIC_Quy-tac-BH-tu-nguyen-xe-o-to_2025.md
6. thuat-ngu-bao-hiem-phi-nhan-tho.md
"""

import os
import re
import asyncio
from typing import List, Dict, Any, Tuple
import configparser

# Load config
config = configparser.ConfigParser()
config.read('../config/insurance_config.ini')
for key in config['DEFAULT']:
    os.environ[key.upper()] = str(config['DEFAULT'][key])

from neo4j import AsyncGraphDatabase

class LegalDocumentParser:
    """Parser t·ªïng h·ª£p cho c√°c lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t"""

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.filename = os.path.basename(file_path)

    def parse_frontmatter(self, content: str) -> Dict[str, Any]:
        """Parse YAML frontmatter"""
        frontmatter = {}
        lines = content.split('\n')

        if lines[0].strip() == '---':
            i = 1
            while i < len(lines) and lines[i].strip() != '---':
                line = lines[i].strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    frontmatter[key.strip()] = value.strip().strip('"')
                i += 1

        return frontmatter

    def parse_legal_document(self, content: str) -> Tuple[str, List[Dict[str, Any]]]:
        """Parse vƒÉn b·∫£n ph√°p lu·∫≠t (ƒêi·ªÅu 1, ƒêi·ªÅu 2, v.v.)"""
        articles = []

        # Pattern cho ƒêi·ªÅu (Article)
        article_pattern = r'###\s*ƒêi·ªÅu\s+(\d+)[\.\s]*(.*?)(?=\n###\s*ƒêi·ªÅu\s+\d+|\n##\s+[A-Z]|\Z)'
        matches = re.findall(article_pattern, content, re.DOTALL | re.MULTILINE)

        for match in matches:
            article_num, article_content = match
            title_match = re.search(r'^([^\n]+)', article_content.strip())
            title = title_match.group(1).strip() if title_match else f"ƒêi·ªÅu {article_num}"

            articles.append({
                'number': int(article_num),
                'title': title,
                'content': article_content.strip(),
                'type': 'legal_article'
            })

        return "legal_document", articles

    def parse_glossary(self, content: str) -> Tuple[str, List[Dict[str, Any]]]:
        """Parse t·ª´ ƒëi·ªÉn thu·∫≠t ng·ªØ A-Z"""
        terms = []

        # Pattern cho t·ª´ng term trong glossary
        # T√¨m c√°c d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng - **term** ‚Äî definition
        term_pattern = r'-\s*\*\*(.+?)\*\*\s*‚Äî\s*(.+?)(?=\n-\s*\*\*|\n##\s+[A-Z]|\Z)'
        matches = re.findall(term_pattern, content, re.DOTALL | re.MULTILINE)

        for match in matches:
            term, definition = match
            terms.append({
                'term': term.strip(),
                'definition': definition.strip(),
                'type': 'glossary_term'
            })

        return "glossary", terms

    def parse_file(self) -> Dict[str, Any]:
        """Parse file theo lo·∫°i"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Parse frontmatter
        frontmatter = self.parse_frontmatter(content)

        # X√°c ƒë·ªãnh lo·∫°i file v√† parse content
        if 'glossary' in self.filename.lower() or 'thuat-ngu' in self.filename.lower():
            doc_type, items = self.parse_glossary(content)
        else:
            doc_type, items = self.parse_legal_document(content)

        return {
            'metadata': frontmatter,
            'doc_type': doc_type,
            'items': items,
            'full_content': content,
            'filename': self.filename
        }

class Neo4JInsuranceImporter:
    """Import d·ªØ li·ªáu b·∫£o hi·ªÉm v√†o Neo4J"""

    def __init__(self):
        self.driver = AsyncGraphDatabase.driver(
            os.environ["NEO4J_URI"],
            auth=(os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
        )

    async def clear_existing_insurance_data(self):
        """X√≥a d·ªØ li·ªáu b·∫£o hi·ªÉm c≈© (gi·ªØ l·∫°i d·ªØ li·ªáu c≈© ƒë√£ import)"""
        print("üßπ ƒêang x√≥a d·ªØ li·ªáu b·∫£o hi·ªÉm c≈©...")
        async with self.driver.session() as session:
            # Ch·ªâ x√≥a nodes c√≥ label InsuranceDocument, kh√¥ng x√≥a LegalDocument c≈©
            await session.run("MATCH (n:InsuranceDocument) DETACH DELETE n")
            await session.run("MATCH (n:InsuranceArticle) DETACH DELETE n")
            await session.run("MATCH (n:GlossaryTerm) DETACH DELETE n")
        print("‚úÖ ƒê√£ x√≥a d·ªØ li·ªáu b·∫£o hi·ªÉm c≈©")

    async def import_insurance_document(self, parsed_data: Dict[str, Any]):
        """Import document b·∫£o hi·ªÉm"""
        print(f"üìÑ ƒêang import: {parsed_data['filename']}")

        async with self.driver.session() as session:
            metadata = parsed_data['metadata']

            # T·∫°o document node
            doc_result = await session.run("""
                CREATE (d:InsuranceDocument {
                    title: $title,
                    source_title: $source_title,
                    source: $source,
                    language: $language,
                    encoding: $encoding,
                    doc_type: $doc_type,
                    issuer: $issuer,
                    jurisdiction: $jurisdiction,
                    created_at: $created_at,
                    filename: $filename,
                    full_content: $full_content
                })
                RETURN id(d) as doc_id
                """,
                title=metadata.get('title', ''),
                source_title=metadata.get('source_title', ''),
                source=metadata.get('source', ''),
                language=metadata.get('language', ''),
                encoding=metadata.get('encoding', ''),
                doc_type=metadata.get('doc_type', parsed_data['doc_type']),
                issuer=metadata.get('issuer', ''),
                jurisdiction=metadata.get('jurisdiction', ''),
                created_at=metadata.get('created_at', ''),
                filename=parsed_data['filename'],
                full_content=parsed_data['full_content']
            )

            doc_record = await doc_result.single()
            doc_id = doc_record['doc_id']

            print(f"‚úÖ ƒê√£ t·∫°o document node v·ªõi ID: {doc_id}")

            # Import items theo lo·∫°i
            if parsed_data['doc_type'] == 'legal_document':
                await self._import_legal_articles(session, doc_id, parsed_data['items'])
            elif parsed_data['doc_type'] == 'glossary':
                await self._import_glossary_terms(session, doc_id, parsed_data['items'])

    async def _import_legal_articles(self, session, doc_id: int, articles: List[Dict[str, Any]]):
        """Import c√°c ƒëi·ªÅu lu·∫≠t"""
        for article in articles:
            await session.run("""
                MATCH (d:InsuranceDocument)
                WHERE id(d) = $doc_id
                CREATE (d)-[:HAS_ARTICLE]->(a:InsuranceArticle {
                    number: $number,
                    title: $title,
                    content: $content,
                    type: $type
                })
                """,
                doc_id=doc_id,
                number=article['number'],
                title=article['title'],
                content=article['content'],
                type=article['type']
            )

        print(f"‚úÖ ƒê√£ import {len(articles)} ƒëi·ªÅu lu·∫≠t")

    async def _import_glossary_terms(self, session, doc_id: int, terms: List[Dict[str, Any]]):
        """Import t·ª´ ƒëi·ªÉn thu·∫≠t ng·ªØ"""
        for term in terms:
            await session.run("""
                MATCH (d:InsuranceDocument)
                WHERE id(d) = $doc_id
                CREATE (d)-[:HAS_TERM]->(t:GlossaryTerm {
                    term: $term,
                    definition: $definition,
                    type: $type
                })
                """,
                doc_id=doc_id,
                term=term['term'],
                definition=term['definition'],
                type=term['type']
            )

        print(f"‚úÖ ƒê√£ import {len(terms)} thu·∫≠t ng·ªØ")

    async def create_indexes(self):
        """T·∫°o indexes cho performance"""
        print("üîç ƒêang t·∫°o indexes cho d·ªØ li·ªáu b·∫£o hi·ªÉm...")

        async with self.driver.session() as session:
            await session.run("CREATE INDEX insurance_doc_title IF NOT EXISTS FOR (d:InsuranceDocument) ON (d.title)")
            await session.run("CREATE INDEX insurance_article_number IF NOT EXISTS FOR (a:InsuranceArticle) ON (a.number)")
            await session.run("CREATE INDEX glossary_term IF NOT EXISTS FOR (t:GlossaryTerm) ON (t.term)")

        print("‚úÖ ƒê√£ t·∫°o indexes")

    async def get_statistics(self):
        """L·∫•y th·ªëng k√™ d·ªØ li·ªáu ƒë√£ import"""
        async with self.driver.session() as session:
            # ƒê·∫øm s·ªë l∆∞·ª£ng documents
            doc_result = await session.run("MATCH (d:InsuranceDocument) RETURN count(d) as count")
            doc_count = doc_result.single()['count']

            # ƒê·∫øm s·ªë l∆∞·ª£ng articles
            article_result = await session.run("MATCH (a:InsuranceArticle) RETURN count(a) as count")
            article_count = article_result.single()['count']

            # ƒê·∫øm s·ªë l∆∞·ª£ng terms
            term_result = await session.run("MATCH (t:GlossaryTerm) RETURN count(t) as count")
            term_count = term_result.single()['count']

            return {
                'documents': doc_count,
                'articles': article_count,
                'terms': term_count
            }

    async def close(self):
        """ƒê√≥ng connection"""
        await self.driver.close()

def get_insurance_files():
    """L·∫•y danh s√°ch files b·∫£o hi·ªÉm c·∫ßn import"""
    base_path = "/Volumes/data/data-tong/data-chatbot/DATACHUAN"
    files_to_import = [
        "04-2021-TTBTC_B·∫£o hi·ªÉm b·∫Øt bu·ªôc xe c∆° gi∆°i.md",
        "MIC - Quy t·∫Øc b·∫£o hi·ªÉm du l·ªãch trong n∆∞·ªõc.md",
        "MIC_CARE_Quy_tac_bao_hiem_suc_khoe_toan_dien_2025.md",
        "MIC_Quy-tac-BH-Tai-nan-con-nguoi_2025.md",
        "MIC_Quy-tac-BH-tu-nguyen-xe-o-to_2025.md",
        "thuat-ngu-bao-hiem-phi-nhan-tho.md"
    ]

    return [os.path.join(base_path, f) for f in files_to_import if os.path.exists(os.path.join(base_path, f))]

async def main():
    """Main async function"""
    print("üèõÔ∏è  Import t·∫•t c·∫£ vƒÉn b·∫£n ph√°p lu·∫≠t b·∫£o hi·ªÉm v√†o Neo4J")
    print("=" * 60)

    # L·∫•y danh s√°ch files
    insurance_files = get_insurance_files()
    print(f"üìÅ T√¨m th·∫•y {len(insurance_files)} files c·∫ßn import:")
    for i, file_path in enumerate(insurance_files, 1):
        print(f"  {i}. {os.path.basename(file_path)}")

    if not insurance_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y files n√†o ƒë·ªÉ import!")
        return

    # Kh·ªüi t·∫°o importer
    importer = Neo4JInsuranceImporter()

    try:
        # Clear d·ªØ li·ªáu c≈©
        await importer.clear_existing_insurance_data()

        # Import t·ª´ng file
        total_files = len(insurance_files)
        for i, file_path in enumerate(insurance_files, 1):
            print(f"\nüîÑ [{i}/{total_files}] Processing: {os.path.basename(file_path)}")

            # Parse file
            parser = LegalDocumentParser(file_path)
            parsed_data = parser.parse_file()

            print(f"   üìä Parsed: {len(parsed_data['items'])} items")

            # Import v√†o Neo4J
            await importer.import_insurance_document(parsed_data)

        # T·∫°o indexes
        await importer.create_indexes()

        # Th·ªëng k√™ cu·ªëi c√πng
        stats = await importer.get_statistics()

        print("\n" + "=" * 60)
        print("‚úÖ IMPORT HO√ÄN TH√ÄNH!")
        print("=" * 60)
        print(f"üìÑ Insurance Documents: {stats['documents']}")
        print(f"üìã Legal Articles: {stats['articles']}")
        print(f"üìö Glossary Terms: {stats['terms']}")
        print("=" * 60)

        # Query examples
        print("üîç Query examples c√≥ th·ªÉ th·ª≠:")
        print("  - MATCH (d:InsuranceDocument)-[:HAS_ARTICLE]->(a:InsuranceArticle) WHERE d.title CONTAINS 'MIC' RETURN d.title, a.title LIMIT 5")
        print("  - MATCH (d:InsuranceDocument)-[:HAS_TERM]->(t:GlossaryTerm) WHERE t.term CONTAINS 'b·∫£o hi·ªÉm' RETURN t.term, t.definition LIMIT 3")
        print("  - MATCH (d:InsuranceDocument {filename: '04-2021-TTBTC_B·∫£o hi·ªÉm b·∫Øt bu·ªôc xe c∆° gi∆°i.md'})-[:HAS_ARTICLE]->(a:InsuranceArticle) RETURN a.title")

    except Exception as e:
        print(f"‚ùå L·ªói import: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        await importer.close()

def run_main():
    """Wrapper ƒë·ªÉ ch·∫°y main v·ªõi asyncio"""
    asyncio.run(main())

if __name__ == "__main__":
    run_main()
