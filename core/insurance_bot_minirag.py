#!/usr/bin/env python3
"""
Insurance Bot sá»­ dá»¥ng MiniRAG framework thay vÃ¬ Neo4J trá»±c tiáº¿p
"""

import os
import sys
import asyncio
import hashlib
import time
from typing import Dict, List, Optional

# Get base directory (works in both local and Docker)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(BASE_DIR, 'MiniRAG'))

# Load config
import configparser
config = configparser.ConfigParser()
config_path = os.path.join(BASE_DIR, 'config', 'insurance_config.ini')
if os.path.exists(config_path):
    config.read(config_path)
    # Set environment variables from config (only if config exists)
    if 'DEFAULT' in config:
        for key in config['DEFAULT']:
            # Only set if not already in environment
            if key.upper() not in os.environ:
                os.environ[key.upper()] = str(config['DEFAULT'][key])

from minirag import MiniRAG, QueryParam
from minirag.llm import gpt_4o_mini_complete
from minirag.utils import EmbeddingFunc
from openai import AsyncOpenAI

class EmbeddingCache:
    """Cache cho embeddings Ä‘á»ƒ trÃ¡nh gá»i API láº·p láº¡i"""

    def __init__(self, ttl_seconds: int = 3600):  # 1 giá» TTL
        self.cache: Dict[str, Dict] = {}
        self.ttl_seconds = ttl_seconds

    def _get_cache_key(self, text: str) -> str:
        """Táº¡o cache key tá»« text"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """Láº¥y embedding tá»« cache náº¿u cÃ²n há»£p lá»‡"""
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if time.time() - entry['timestamp'] < self.ttl_seconds:
                print(f"ğŸ“‹ Cache hit for: {text[:50]}...")
                return entry['embedding']
            else:
                # Cache expired
                del self.cache[cache_key]
        return None

    def set(self, text: str, embedding: List[float]):
        """LÆ°u embedding vÃ o cache"""
        cache_key = self._get_cache_key(text)
        self.cache[cache_key] = {
            'embedding': embedding,
            'timestamp': time.time()
        }
        print(f"ğŸ’¾ Cached embedding for: {text[:50]}...")

    def clear_expired(self):
        """XÃ³a cache entries Ä‘Ã£ háº¿t háº¡n"""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if current_time - entry['timestamp'] >= self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
        if expired_keys:
            print(f"ğŸ—‘ï¸ Cleared {len(expired_keys)} expired cache entries")

# Global embedding cache
embedding_cache = EmbeddingCache()

# Singleton OpenAI client Ä‘á»ƒ reuse connection (tá»‘i Æ°u performance)
_openai_client: Optional[AsyncOpenAI] = None

def get_openai_client() -> AsyncOpenAI:
    """Get or create singleton OpenAI client vá»›i connection pooling"""
    global _openai_client
    if _openai_client is None:
        api_key = os.environ.get('OPENAI_API_KEY') or config.get('DEFAULT', 'OPENAI_API_KEY', fallback=None)
        base_url = os.environ.get('OPENAI_BASE_URL') or os.environ.get('OPENAI_API_BASE') or config.get('DEFAULT', 'OPENAI_BASE_URL', fallback=None)
        
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables or config file")
        
        # Tá»‘i Æ°u: reuse connections, timeout ngáº¯n hÆ¡n
        _openai_client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=30.0,  # Timeout 30s thay vÃ¬ default
            max_retries=2,  # Giáº£m retries Ä‘á»ƒ fail fast
        )
        print("âœ… OpenAI client initialized (singleton, connection pooling enabled)")
    return _openai_client

async def get_openai_embedding_func(texts):
    """Async OpenAI embedding function cho MiniRAG vá»›i cache vÃ  connection reuse"""
    try:
        # Check cache cho tá»«ng text
        cached_embeddings = []
        texts_to_fetch = []
        cache_indices = []

        for i, text in enumerate(texts):
            cached = embedding_cache.get(text)
            if cached is not None:
                cached_embeddings.append((i, cached))
            else:
                texts_to_fetch.append(text)
                cache_indices.append(i)

        # Chá»‰ gá»i API cho texts chÆ°a cÃ³ trong cache
        if texts_to_fetch:
            print(f"ğŸ” Fetching embeddings for {len(texts_to_fetch)} texts...")
            embedding_model = os.environ.get('EMBEDDING_MODEL') or config.get('DEFAULT', 'EMBEDDING_MODEL', fallback='text-embedding-3-small')
            
            # Reuse singleton client (connection pooling)
            client = get_openai_client()

            # Batch request vá»›i timeout ngáº¯n
            response = await client.embeddings.create(
                input=texts_to_fetch,
                model=embedding_model
            )

            fetched_embeddings = [data.embedding for data in response.data]

            # Cache cÃ¡c embeddings má»›i
            for text, embedding in zip(texts_to_fetch, fetched_embeddings):
                embedding_cache.set(text, embedding)
        else:
            fetched_embeddings = []

        # Káº¿t há»£p cached vÃ  fetched embeddings theo thá»© tá»± gá»‘c
        result = [None] * len(texts)

        # Äiá»n cached embeddings
        for idx, embedding in cached_embeddings:
            result[idx] = embedding

        # Äiá»n fetched embeddings
        for i, embedding in enumerate(fetched_embeddings):
            result[cache_indices[i]] = embedding

        return result

    except Exception as e:
        print(f"âŒ OpenAI embedding error: {e}")
        # Return dummy embeddings if OpenAI fails
        return [[0.1] * 1536 for _ in texts]

# Insurance Bot Prompt
INSURANCE_BOT_PROMPT = """
### VAI TRÃ’ VÃ€ Bá»I Cáº¢NH 

Báº¡n lÃ  nhÃ¢n viÃªn tÆ° váº¥n chuyÃªn nghiá»‡p cá»§a CÃ´ng ty Ä‘áº¡i lÃ½ báº£o hiá»ƒm FISS. 

Nhiá»‡m vá»¥ chÃ­nh cá»§a báº¡n lÃ :

- TÆ° váº¥n vÃ  giáº£i Ä‘Ã¡p má»i tháº¯c máº¯c vá» cÃ¡c sáº£n pháº©m báº£o hiá»ƒm

- Há»— trá»£ khÃ¡ch hÃ ng tra cá»©u thÃ´ng tin há»£p Ä‘á»“ng, quyá»n lá»£i báº£o hiá»ƒm

- HÆ°á»›ng dáº«n quy trÃ¬nh mua báº£o hiá»ƒm, ná»™p há»“ sÆ¡ bá»“i thÆ°á»ng

- Cung cáº¥p bÃ¡o giÃ¡ vÃ  tÆ° váº¥n sáº£n pháº©m phÃ¹ há»£p vá»›i nhu cáº§u khÃ¡ch hÃ ng

### PHONG CÃCH GIAO TIáº¾P

- ThÃ¢n thiá»‡n, nhiá»‡t tÃ¬nh vÃ  chuyÃªn nghiá»‡p

- Sá»­ dá»¥ng ngÃ´n ngá»¯ dá»… hiá»ƒu, trÃ¡nh thuáº­t ngá»¯ phá»©c táº¡p (hoáº·c giáº£i thÃ­ch rÃµ náº¿u cáº§n dÃ¹ng)

- Láº¯ng nghe vÃ  tháº¥u hiá»ƒu nhu cáº§u khÃ¡ch hÃ ng

- LuÃ´n káº¿t thÃºc cÃ¢u tráº£ lá»i báº±ng cÃ¢u há»i/ghi chÃº tÃ­ch cá»±c Ä‘á»ƒ duy trÃ¬ cuá»™c há»™i thoáº¡i

### NGUYÃŠN Táº®C TRá»¢ GIÃšP

1. **LÃ m rÃµ nhu cáº§u**: Náº¿u cÃ¢u há»i chÆ°a rÃµ rÃ ng, hÃ£y Ä‘áº·t cÃ¢u há»i Ä‘á»ƒ hiá»ƒu Ä‘Ãºng Ã½ khÃ¡ch hÃ ng

   - VÃ­ dá»¥: "Anh/chá»‹ quan tÃ¢m Ä‘áº¿n báº£o hiá»ƒm xe mÃ¡y hay Ã´ tÃ´ áº¡?"

   - VÃ­ dá»¥: "Äá»ƒ tÆ° váº¥n chÃ­nh xÃ¡c, cho em há»i anh/chá»‹ muá»‘n má»©c phÃ­ báº£o hiá»ƒm khoáº£ng bao nhiÃªu?"

2. **Tráº£ lá»i chÃ­nh xÃ¡c**: Chá»‰ cung cáº¥p thÃ´ng tin dá»±a trÃªn kiáº¿n thá»©c Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o vá»:

   - Sáº£n pháº©m báº£o hiá»ƒm cá»§a cÃ´ng ty

   - Quy Ä‘á»‹nh phÃ¡p luáº­t vá» báº£o hiá»ƒm Viá»‡t Nam

   - Quy trÃ¬nh vÃ  chÃ­nh sÃ¡ch cá»§a cÃ´ng ty

3. **Pháº£n há»“i khi khÃ´ng biáº¿t**: Náº¿u cÃ¢u há»i náº±m ngoÃ i pháº¡m vi kiáº¿n thá»©c:

   "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:

   - LiÃªn há»‡ hotline: 0385 10 10 18

   - Email: cskh@fiss.com.vn

   - Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."

4. **Xá»­ lÃ½ yÃªu cáº§u phá»©c táº¡p**: Vá»›i cÃ¡c váº¥n Ä‘á» vá»:

   - Bá»“i thÆ°á»ng báº£o hiá»ƒm cá»¥ thá»ƒ

   - Tranh cháº¥p há»£p Ä‘á»“ng

   - Thay Ä‘á»•i thÃ´ng tin há»£p Ä‘á»“ng quan trá»ng

   â†’ HÆ°á»›ng dáº«n khÃ¡ch hÃ ng káº¿t ná»‘i vá»›i bá»™ pháº­n chuyÃªn trÃ¡ch

### GIá»šI Háº N VÃ€ RANH GIá»šI

1. **KHÃ”NG tiáº¿t lá»™ dá»¯ liá»‡u há»‡ thá»‘ng**: 

   - KhÃ´ng Ä‘á» cáº­p Ä‘áº¿n viá»‡c báº¡n cÃ³ quyá»n truy cáº­p vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘Ã o táº¡o

   - KhÃ´ng nÃ³i "trong dá»¯ liá»‡u cá»§a tÃ´i cÃ³...", thay vÃ o Ä‘Ã³ nÃ³i "theo quy Ä‘á»‹nh hiá»‡n hÃ nh..." hoáº·c "theo chÃ­nh sÃ¡ch cÃ´ng ty..."

2. **Duy trÃ¬ focus**: 

   - Náº¿u khÃ¡ch hÃ ng há»i vá» chá»§ Ä‘á» khÃ´ng liÃªn quan (thá»i tiáº¿t, chÃ­nh trá»‹, giáº£i trÃ­...):

     "Em hiá»ƒu anh/chá»‹ quan tÃ¢m, nhÆ°ng chuyÃªn mÃ´n cá»§a em lÃ  tÆ° váº¥n vá» báº£o hiá»ƒm. Anh/chá»‹ cÃ³ tháº¯c máº¯c gÃ¬ vá» cÃ¡c sáº£n pháº©m báº£o hiá»ƒm cá»§a cÃ´ng ty khÃ´ng áº¡?"

3. **Chá»‰ dá»±a vÃ o kiáº¿n thá»©c Ä‘Æ°á»£c Ä‘Ã o táº¡o**:

   - KhÃ´ng tá»± suy diá»…n hoáº·c Ä‘Æ°a ra thÃ´ng tin khÃ´ng cháº¯c cháº¯n

   - KhÃ´ng so sÃ¡nh vá»›i sáº£n pháº©m cá»§a Ä‘á»‘i thá»§ (trá»« khi cÃ³ dá»¯ liá»‡u chÃ­nh thá»©c)

4. **TUYá»†T Äá»I KHÃ”NG**:

   - Hiá»ƒn thá»‹ pháº§n "References", "Nguá»“n tÃ i liá»‡u", hoáº·c tÃªn file (.md, .pdf)

   - Liá»‡t kÃª [1], [2], [3] á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i

   - ÄÆ°a ra lá»i khuyÃªn phÃ¡p lÃ½ hoáº·c tÃ i chÃ­nh chuyÃªn sÃ¢u

   - Cam káº¿t vá» káº¿t quáº£ bá»“i thÆ°á»ng cá»¥ thá»ƒ mÃ  chÆ°a cÃ³ tháº©m Ä‘á»‹nh

### Cáº¤U TRÃšC CÃ‚U TRáº¢ Lá»œI LÃ TÆ¯á»NG

1. **ChÃ o há»i/Thá»«a nháº­n cÃ¢u há»i**: "Dáº¡, em xin giáº£i Ä‘Ã¡p tháº¯c máº¯c cá»§a anh/chá»‹ vá»..."

2. **Ná»™i dung chÃ­nh**: Tráº£ lá»i trá»±c tiáº¿p, sÃºc tÃ­ch, cÃ³ cáº¥u trÃºc

3. **ThÃ´ng tin bá»• sung** (náº¿u cáº§n): VÃ­ dá»¥, lÆ°u Ã½ quan trá»ng

4. **Káº¿t thÃºc tÃ­ch cá»±c**: CÃ¢u há»i má»Ÿ hoáº·c lá»i khuyÃªn há»¯u Ã­ch

   - "Anh/chá»‹ cÃ²n tháº¯c máº¯c gÃ¬ khÃ¡c em cÃ³ thá»ƒ há»— trá»£ khÃ´ng áº¡?"

   - "Em cÃ³ thá»ƒ tÆ° váº¥n thÃªm vá» gÃ³i báº£o hiá»ƒm phÃ¹ há»£p vá»›i nhu cáº§u cá»§a anh/chá»‹ náº¿u muá»‘n áº¡!"

### VÃ Dá»¤ TÆ¯Æ NG TÃC

**Tá»‘t:**

KhÃ¡ch: "Xe mÃ¡y tÃ´i bá»‹ tai náº¡n, báº£o hiá»ƒm cÃ³ chi tráº£ khÃ´ng?"

Bot: "Dáº¡, em xin giáº£i Ä‘Ã¡p áº¡. Báº£o hiá»ƒm báº¯t buá»™c trÃ¡ch nhiá»‡m dÃ¢n sá»± xe mÃ¡y sáº½ chi tráº£ cho:

- Thiá»‡t háº¡i vá» ngÆ°á»i vÃ  tÃ i sáº£n cá»§a bÃªn thá»© ba (ngÆ°á»i bá»‹ náº¡n)

- KhÃ´ng bá»“i thÆ°á»ng cho chÃ­nh xe mÃ¡y vÃ  chá»§ xe gÃ¢y tai náº¡n

Náº¿u anh/chá»‹ muá»‘n xe mÃ¡y Ä‘Æ°á»£c báº£o hiá»ƒm khi bá»‹ hÆ° há»ng, anh/chá»‹ cáº§n mua thÃªm gÃ³i báº£o hiá»ƒm váº­t cháº¥t xe (báº£o hiá»ƒm tá»± nguyá»‡n) áº¡.

Xe cá»§a anh/chá»‹ hiá»‡n cÃ³ mua báº£o hiá»ƒm tá»± nguyá»‡n khÃ´ng áº¡? Em cÃ³ thá»ƒ tÆ° váº¥n thÃªm náº¿u anh/chá»‹ quan tÃ¢m!"

**KhÃ´ng tá»‘t:**

KhÃ¡ch: "Xe mÃ¡y tÃ´i bá»‹ tai náº¡n, báº£o hiá»ƒm cÃ³ chi tráº£ khÃ´ng?"

Bot: "CÃ³, báº£o hiá»ƒm sáº½ chi tráº£.

### Xá»¬ LÃ CÃC TÃŒNH HUá»NG Äáº¶C BIá»†T

**1. KhÃ¡ch hÃ ng tá»©c giáº­n:**

"Em ráº¥t hiá»ƒu sá»± bá»©c xÃºc cá»§a anh/chá»‹. Em sáº½ cá»‘ gáº¯ng há»— trá»£ tá»‘t nháº¥t. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nhanh chÃ³ng, anh/chá»‹ vui lÃ²ng cho em biáº¿t [thÃ´ng tin cáº§n thiáº¿t]..."

**2. YÃªu cáº§u ngoÃ i kháº£ nÄƒng:**

"Em xin lá»—i vÃ¬ chÆ°a thá»ƒ há»— trá»£ váº¥n Ä‘á» nÃ y qua chat. Äá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ nhanh chÃ³ng vÃ  chÃ­nh xÃ¡c, em xin chuyá»ƒn anh/chá»‹ sang bá»™ pháº­n CSKH qua Zalo: 033 6691379."

**3. ThÃ´ng tin nháº¡y cáº£m:**

"Äá»ƒ báº£o máº­t thÃ´ng tin cÃ¡ nhÃ¢n, em khÃ´ng thá»ƒ xá»­ lÃ½ thÃ´ng tin nÃ y qua chat áº¡. Anh/chá»‹ vui lÃ²ng liÃªn há»‡ trá»±c tiáº¿p vá»›i chÃºng em qua hotline 0385 10 10 18 hoáº·c Ä‘áº¿n vÄƒn phÃ²ng Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ an toÃ n hÆ¡n áº¡."

### HÆ°á»›ng dáº«n mua hÃ ng

Khi khÃ¡ch há»i cÃ¡ch mua sáº£n pháº©m, tráº£ lá»i quy trÃ¬nh mua hÃ ng cá»§a sáº£n pháº©m Ä‘Ã³ theo format:

**Quy trÃ¬nh mua [TÃªn sáº£n pháº©m]:**

- BÆ°á»›c 1: [HÃ nh Ä‘á»™ng Ä‘áº§u tiÃªn]

- BÆ°á»›c 2: [HÃ nh Ä‘á»™ng tiáº¿p theo]

- BÆ°á»›c 3: [HÃ nh Ä‘á»™ng tiáº¿p theo]

- BÆ°á»›c 4: [HoÃ n táº¥t]

**VÃ­ dá»¥ - Mua Báº£o hiá»ƒm báº¯t buá»™c xe mÃ¡y:**

- BÆ°á»›c 1: Má»Ÿ app Fiss â†’ chá»n sáº£n pháº©m â†’ nháº­n bÃ¡o giÃ¡

- BÆ°á»›c 2: Nháº­p sá»‘ khung, sá»‘ mÃ¡y

- BÆ°á»›c 3: Xem láº¡i vÃ  thanh toÃ¡n

- BÆ°á»›c 4: Giáº¥y chá»©ng nháº­n Ä‘iá»‡n tá»­ tá»± Ä‘á»™ng lÆ°u trong app

Chá»‰ liá»‡t kÃª cÃ¡c bÆ°á»›c thá»±c hiá»‡n, khÃ´ng giáº£i thÃ­ch thÃªm.

### chÃº Ã½

- Náº¿u cÃ¢u há»i Ä‘Ã£ tá»«ng tráº£ lá»i hÃ£y láº¥y tá»« bá»™ nhá»› ra Ä‘á»ƒ tráº£ lá»i khÃ´ng cáº§n truy váº¥n lÃ¢u

### LÆ¯U Ã QUAN TRá»ŒNG

- LuÃ´n Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c 100% vá» sá»‘ tiá»n, ngÃ y thÃ¡ng, Ä‘iá»u khoáº£n

- KhÃ´ng tá»± Ã½ sá»­a Ä‘á»•i hoáº·c giáº£i thÃ­ch sai cÃ¡c quy Ä‘á»‹nh phÃ¡p luáº­t

- Khi Ä‘á» cáº­p sá»‘ liá»‡u, pháº£i rÃµ rÃ ng (vÃ­ dá»¥: "66.000 VNÄ/nÄƒm" thay vÃ¬ "khoáº£ng 60k")

- LuÃ´n cáº­p nháº­t thÃ´ng tin theo quy Ä‘á»‹nh má»›i nháº¥t cá»§a Bá»™ TÃ i chÃ­nh
"""

class InsuranceBotMiniRAG:
    """Bot sá»­ dá»¥ng MiniRAG framework"""

    def __init__(self):
        print("ğŸš€ Initializing Insurance Bot with MiniRAG...")

        # Æ¯u tiÃªn Ä‘á»c tá»« environment variables
        working_dir = os.environ.get('WORKING_DIR') or config.get('DEFAULT', 'WORKING_DIR', fallback='./insurance_rag')
        # Normalize working_dir: náº¿u lÃ  Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i chá»©a /Volumes, chuyá»ƒn thÃ nh relative
        if working_dir.startswith('/Volumes'):
            # Extract relative path from /Volumes/data/MINIRAG/logs/insurance_rag
            if 'logs/insurance_rag' in working_dir:
                working_dir = './logs/insurance_rag'
            else:
                working_dir = './insurance_rag'
        # Äáº£m báº£o working_dir lÃ  relative path trong container
        if not working_dir.startswith('./'):
            working_dir = './' + working_dir.lstrip('/')
        
        # Tá»‘i Æ°u: Giá»¯ max_tokens Ä‘á»§ Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§ (1200 cho báº£o hiá»ƒm cáº§n chi tiáº¿t)
        llm_max_tokens = int(os.environ.get('OPENAI_LLM_MAX_TOKENS') or config.get('DEFAULT', 'OPENAI_LLM_MAX_TOKENS', fallback='1200'))
        llm_model = os.environ.get('OPENAI_LLM_MODEL') or config.get('DEFAULT', 'OPENAI_LLM_MODEL', fallback='gpt-4o-mini')
        
        print(f"ğŸ“ Working directory: {working_dir}")

        self.rag = MiniRAG(
            working_dir=working_dir,
            llm_model_func=gpt_4o_mini_complete,
            llm_model_max_token_size=llm_max_tokens,
            llm_model_name=llm_model,
            llm_model_kwargs={
                "system_prompt": INSURANCE_BOT_PROMPT
            },
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                max_token_size=1000,
                func=get_openai_embedding_func,
            ),
        )

        # Cache cho response vá»›i TTL
        self.response_cache: Dict[str, Dict] = {}
        self.cache_ttl = 3600  # 1 giá»
        
        # Pre-warm cache vá»›i common queries (tá»‘i Æ°u tá»‘c Ä‘á»™)
        self._pre_warm_cache()
        
        print("âœ… Insurance Bot with MiniRAG initialized!")
    
    def _pre_warm_cache(self):
        """Pre-warm cache vá»›i common queries Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™"""
        common_queries = [
            "Báº£o hiá»ƒm xe mÃ¡y lÃ  gÃ¬?",
            "PhÃ­ báº£o hiá»ƒm xe mÃ¡y bao nhiÃªu?",
            "Quy trÃ¬nh mua báº£o hiá»ƒm xe mÃ¡y?",
            "Báº£o hiá»ƒm sá»©c khá»e lÃ  gÃ¬?",
            "Báº£o hiá»ƒm báº¯t buá»™c lÃ  gÃ¬?",
        ]
        
        # Pre-compute embeddings cho common queries (async, khÃ´ng block)
        async def pre_warm_embeddings():
            try:
                for query in common_queries:
                    await get_openai_embedding_func([query])
                print(f"âœ… Pre-warmed cache vá»›i {len(common_queries)} common queries")
            except Exception as e:
                print(f"âš ï¸ Pre-warm cache error: {e}")
        
        # Cháº¡y pre-warm trong background (khÃ´ng block initialization)
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # Náº¿u loop Ä‘ang cháº¡y, schedule task
                asyncio.create_task(pre_warm_embeddings())
            else:
                # Náº¿u khÃ´ng, cháº¡y sync
                loop.run_until_complete(pre_warm_embeddings())
        except Exception:
            # Náº¿u khÃ´ng cÃ³ event loop, bá» qua pre-warm
            pass

    def extract_keywords(self, question: str):
        """TrÃ­ch xuáº¥t tá»« khÃ³a tá»« cÃ¢u há»i"""
        stop_words = ['lÃ ', 'cÃ¡i', 'Ä‘Ã³', 'Ä‘Ã¢y', 'á»Ÿ', 'táº¡i', 'vÃ ', 'hoáº·c', 'nhÆ°', 'tháº¿ nÃ o', 'gÃ¬', 'Ä‘Æ°á»£c', 'cÃ³', 'khÃ´ng']
        words = question.split()
        keywords = []

        for word in words:
            if len(word) > 2 and word not in stop_words:
                keywords.append(word)

        if not keywords:
            keywords = [question]

        insurance_terms = ['báº£o hiá»ƒm', 'báº£o', 'hiá»ƒm', 'xe', 'mÃ¡y', 'Ã´ tÃ´', 'phÆ°Æ¡ng tiá»‡n', 'thiá»‡t háº¡i', 'tai náº¡n', 'sá»©c khá»e', 'du lá»‹ch', 'nhÃ¢n thá»']
        prioritized_keywords = []
        for term in insurance_terms:
            if term in question:
                prioritized_keywords.append(term)

        final_keywords = prioritized_keywords + [k for k in keywords if k not in prioritized_keywords]
        return final_keywords[:5]

    async def chat(self, question: str) -> str:
        """Chat vá»›i bot sá»­ dá»¥ng MiniRAG - Tá»‘i Æ°u cho tá»‘c Ä‘á»™ < 15s"""
        start_time = time.time()
        print(f"ğŸ‘¤ Question: {question}")

        # Check cache first
        cache_key = question.lower().strip()
        if cache_key in self.response_cache:
            entry = self.response_cache[cache_key]
            if time.time() - entry['timestamp'] < self.cache_ttl:
                print(f"ğŸ“‹ Using cached response (saved {time.time() - entry['timestamp']:.1f}s ago)")
                return entry['answer']
            else:
                # Cache expired
                del self.response_cache[cache_key]

        print("ğŸ” Querying MiniRAG (optimized for speed + accuracy)...")

        try:
            # Tá»‘i Æ°u cÃ¢n báº±ng: Tá»‘c Ä‘á»™ + Äá»™ chÃ­nh xÃ¡c (quan trá»ng cho lÄ©nh vá»±c báº£o hiá»ƒm)
            # - top_k: 8-10 (Ä‘á»§ Ä‘á»ƒ cÃ³ káº¿t quáº£ chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§)
            # - max_token_for_text_unit: 2500 (Ä‘á»§ context, khÃ´ng máº¥t tá»«)
            # - Light mode: CÃ³ graph context, chÃ­nh xÃ¡c hÆ¡n naive mode
            # - Tá»‘i Æ°u báº±ng caching, connection pooling, khÃ´ng giáº£m cháº¥t lÆ°á»£ng
            query_param = QueryParam(
                mode="light",  # Light mode: cÃ³ graph context, chÃ­nh xÃ¡c hÆ¡n naive
                top_k=8,  # Äá»§ Ä‘á»ƒ cÃ³ káº¿t quáº£ chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ (khÃ´ng giáº£m)
                max_token_for_text_unit=2500,  # Äá»§ context, khÃ´ng máº¥t tá»«
                max_token_for_node_context=400,  # Äá»§ context cho entities
                max_token_for_local_context=2000,  # Äá»§ context cho local
                max_token_for_global_context=2000,  # Äá»§ context cho global
            )
            
            query_start = time.time()
            try:
                answer = await self.rag.aquery(question, param=query_param)
                query_time = time.time() - query_start
            except Exception as light_error:
                # Náº¿u light mode fail, fallback sang naive mode vá»›i top_k Ä‘á»§
                print(f"âš ï¸ Light mode failed: {light_error}, trying naive mode with top_k=8...")
                query_param = QueryParam(
                    mode="naive",
                    top_k=8,  # Váº«n giá»¯ Ä‘á»§ Ä‘á»ƒ chÃ­nh xÃ¡c
                    max_token_for_text_unit=2500,  # Váº«n giá»¯ Ä‘á»§ context
                )
                query_start = time.time()
                answer = await self.rag.aquery(question, param=query_param)
                query_time = time.time() - query_start

            total_time = time.time() - start_time
            print(f"â±ï¸ Query time: {query_time:.2f}s, Total time: {total_time:.2f}s")

            # Cache response vá»›i timestamp
            self.response_cache[cache_key] = {
                'answer': answer,
                'timestamp': time.time()
            }

            # Cleanup expired cache entries (keep cache size manageable)
            if len(self.response_cache) > 100:
                current_time = time.time()
                expired_keys = [
                    key for key, entry in self.response_cache.items()
                    if current_time - entry['timestamp'] >= self.cache_ttl
                ]
                for key in expired_keys[:50]:  # Remove up to 50 expired entries
                    del self.response_cache[key]

            print(f"ğŸ’¬ MiniRAG Answer: {answer[:100]}...")
            return answer

        except Exception as e:
            print(f"âŒ MiniRAG query error: {e}")
            import traceback
            traceback.print_exc()
            return f"Xin lá»—i, hiá»‡n táº¡i há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Anh/chá»‹ vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ hotline 0385 10 10 18 Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ áº¡."

    async def close(self):
        """Close resources"""
        print("ğŸ‘‹ Insurance Bot closed")

async def main():
    """Main function for interactive chat"""
    print("ğŸ¤– INSURANCE BOT - Sá»­ dá»¥ng MiniRAG Framework")
    print("=" * 60)

    bot = InsuranceBotMiniRAG()

    try:
        print("ğŸ’¬ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i dá»‹ch vá»¥ tÆ° váº¥n báº£o hiá»ƒm FISS!")
        print("ğŸ“ HÃ£y Ä‘áº·t cÃ¢u há»i vá» báº£o hiá»ƒm, em sáº½ há»— trá»£ báº¡n ngay áº¡.")
        print("âŒ GÃµ 'quit' Ä‘á»ƒ thoÃ¡t")
        print()

        while True:
            try:
                question = input("ğŸ‘¤ Báº¡n: ").strip()

                if question.lower() in ['quit', 'exit', 'bye']:
                    print("ğŸ’¬ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥ tÆ° váº¥n cá»§a FISS!")
                    print("ğŸ“ Náº¿u cáº§n há»— trá»£ thÃªm, hÃ£y liÃªn há»‡ hotline 0385 10 10 18 nhÃ©!")
                    break

                if not question:
                    continue

                answer = await bot.chat(question)
                print(f"ğŸ’¬ FISS Bot: {answer}")
                print()

            except KeyboardInterrupt:
                print("\nğŸ’¬ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")
                continue

    finally:
        await bot.close()

if __name__ == "__main__":
    asyncio.run(main())
